{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d73c97-da58-46b7-b670-4c557d88d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo wget -O /usr/local/spark/jars/hadoop-aws-3.3.1.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a6ef4-3351-4f4d-af81-c2179aa00669",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo wget -O /usr/local/spark/jars/aws-java-sdk-bundle-1.11.901.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee168401-3f2f-4370-9140-de791f3a78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, IndexToString\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import when, count, col, lit, udf, isnan\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38e5074-3321-4f88-ac53-9c42f8581e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_CUSTOM_ENDPOINT = \"http://storage:9000\"\n",
    "AWS_ACCESS_KEY_ID = \"minioadmin\"\n",
    "AWS_SECRET_ACCESS_KEY = \"minioadmin\"\n",
    "\n",
    "FRAUD_DETECTION_SRC_DIR = \"s3a://ml-data/fraud-data-source\"\n",
    "\n",
    "FRAUD_DETECTION_DB=\"ml_fraud_detection_db\"\n",
    "FRAUD_DETECTION_SRC_TBL=f\"{FRAUD_DETECTION_DB}.tb_fraud\"\n",
    "\n",
    "FRAUD_DETECTION_OUTPUT_DB_DIR = f\"s3a://ml-data/dev/{FRAUD_DETECTION_DB}\"\n",
    "FRAUD_DETECTION_OUTPUT_DIR = f\"{FRAUD_DETECTION_OUTPUT_DB_DIR}/tb_fraud\"\n",
    "LABEL_COLUMN = \"isFraud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6d854c-4c22-4f22-9ac0-aadf04ba0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[6]\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.executor.cores\", \"1\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "conf.set(\"spark.hadoop.parquet.enable.summary-metadata\", \"false\")\n",
    "conf.set(\"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "conf = conf.set(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265a441d-a626-4144-9df9-eaa0ffc50b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_CUSTOM_ENDPOINT)\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "conf=conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f77d86-c32b-4495-9a9b-0f4087656fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"EnrichProcessor\").config(conf=conf).enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ded9c3-d960-490d-940a-fbd6ed4faf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transaction_df = spark.read.csv(f\"{FRAUD_DETECTION_SRC_DIR}/train_transaction.csv\",inferSchema=True, header=True)\n",
    "train_identity_df = spark.read.csv(f\"{FRAUD_DETECTION_SRC_DIR}/train_identity.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6436dc15-3503-4c2d-bcdc-c65b23da44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", '12')\n",
    "union_df = train_transaction_df \\\n",
    "    .join(train_identity_df, on=\"transactionID\", how=\"left\") \\\n",
    "    .orderBy(\"transactionDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a275ae-37a4-489d-a67b-3ac2c52e23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = union_df.select(\n",
    "        ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'dist1',\n",
    "         'P_emaildomain', 'R_emaildomain', 'C1', 'C5',\n",
    "         'C13', 'D1', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V2', 'V3',\n",
    "         'V4', 'V6', 'V8', 'V10',\n",
    "         'V12', 'V15', 'V19', 'V22', 'V23', 'V25', 'V29', 'V35', LABEL_COLUMN]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45bd2fb5-06ba-4fa3-9a09-d454bd03f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_columns(df: DataFrame, label_column_name: str) -> list:\n",
    "    fields = df.schema.fields\n",
    "    cat_columns = list()\n",
    "    for f in fields:\n",
    "        if f.dataType.typeName() == 'string' and f.name != label_column_name:\n",
    "            cat_columns.append(f.name)\n",
    "    return cat_columns\n",
    "\n",
    "def get_numerical_columns(df: DataFrame, label_column_name: str) -> list:\n",
    "    fields = df.schema.fields\n",
    "    num_columns = list()\n",
    "    for f in fields:\n",
    "        if f.dataType.typeName() != 'string' and f.name != label_column_name:\n",
    "            num_columns.append(f.name)\n",
    "    return num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0485cd2d-41ea-4572-8188-3113c889dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(df, columns, strategy=\"mean\"):\n",
    "    df = df.select(columns)\n",
    "    imputed_values = dict()\n",
    "    if strategy == \"mode\":\n",
    "        for c in columns:\n",
    "            calculate_mode(c, df, imputed_values)\n",
    "    elif strategy == \"mean\":\n",
    "        calculate_mean(columns, df, imputed_values)\n",
    "    return imputed_values\n",
    "\n",
    "\n",
    "def calculate_mean(columns, df, imputed_values):\n",
    "    single_cat_df = df.withColumn(\"df_id\", lit(1))\n",
    "    single_cat_df.createOrReplaceTempView(f\"group_df\")\n",
    "\n",
    "    avg_columns = [f\"avg({c}) as {c}\" for c in columns]\n",
    "    avg_columns_str = \",\".join(avg_columns)\n",
    "    group_sql_df = spark.sql(f\"select {avg_columns_str} from group_df group by df_id\")\n",
    "    avg_row = group_sql_df.take(1)[0]\n",
    "    for c in columns:\n",
    "        imputed_values[c] = avg_row[c]\n",
    "\n",
    "\n",
    "def calculate_mode(c, df, imputed_values):\n",
    "    single_cat_df = df.select(c)\n",
    "    group_df = single_cat_df.groupby(c).count()\n",
    "    group_df.createOrReplaceTempView(f\"{c}_group_df\")\n",
    "    group_sql_df = spark.sql(\n",
    "        f\"select {c}, rank() over (partition by count order by count desc) as max_count, count from {c}_group_df where {c} is not null\")\n",
    "    imputed_values[c] = group_sql_df.take(1)[0][c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52ef5aa-1fe9-44d5-a992-c1e0c80bd797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
      "['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'dist1', 'C1', 'C5', 'C13', 'D1', 'D3', 'D4', 'D5', 'D10', 'D11', 'D15', 'V2', 'V3', 'V4', 'V6', 'V8', 'V10', 'V12', 'V15', 'V19', 'V22', 'V23', 'V25', 'V29', 'V35']\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = get_categorical_columns(final_df, LABEL_COLUMN)\n",
    "print(categorical_columns)\n",
    "\n",
    "numerical_columns = get_numerical_columns(final_df, LABEL_COLUMN)\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6881936-ffe4-44fc-85a4-d2cf73dc271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_values = calculate(final_df, categorical_columns, strategy=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e7832fe-3066-4f20-b2c7-d119fbb42cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imputed_final_df = final_df.select(\n",
    "        [(when(isnan(c) | col(c).isNull(), mode_values[c]).otherwise(final_df[c])).alias(c) for c in\n",
    "         categorical_columns] + numerical_columns + [LABEL_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08ee0afd-2c6a-4ab3-b4f0-b45e8202ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values = calculate(cat_imputed_final_df, numerical_columns, strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af0113c-1f26-4cb3-ab9a-c1762162f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputed_final_df = cat_imputed_final_df \\\n",
    "        .select([(when(isnan(c) | col(c).isNull(), mean_values[c]).otherwise(final_df[c])).alias(c) for c in\n",
    "                 numerical_columns] + categorical_columns + [LABEL_COLUMN])\n",
    "\n",
    "num_imputed_final_df.createOrReplaceTempView(\"imputed_final_tmp_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634381bf-a72e-4e3e-9195-cf7e83c688e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_imputed_final_df.coalesce(1).write.parquet(FRAUD_DETECTION_OUTPUT_DIR, mode=\"overwrite\", compression=\"snappy\")\n",
    "table_df = spark.sql(f\"CREATE DATABASE IF NOT EXISTS {FRAUD_DETECTION_DB} LOCATION '{FRAUD_DETECTION_OUTPUT_DB_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56651ff9-fcb6-44d4-a4a5-556cd5458788",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_table_columns = [f\"{field.name} {field.dataType.simpleString()}\" for field in num_imputed_final_df.schema.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fba325a8-1c28-4bb4-8805-b8e1a936f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_table_columns_str = \",\".join(final_output_table_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8af0ee-3048-451a-9861-6d7a30b2ff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TransactionAmt double,card1 double,card2 double,card3 double,card5 double,dist1 double,C1 double,C5 double,C13 double,D1 double,D3 double,D4 double,D5 double,D10 double,D11 double,D15 double,V2 double,V3 double,V4 double,V6 double,V8 double,V10 double,V12 double,V15 double,V19 double,V22 double,V23 double,V25 double,V29 double,V35 double,ProductCD string,card4 string,card6 string,P_emaildomain string,R_emaildomain string,M2 string,M3 string,M4 string,M5 string,M6 string,M7 string,M8 string,M9 string,isFraud int'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output_table_columns_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ca93804-4be8-4e31-87c6-1640daa291bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {FRAUD_DETECTION_SRC_TBL}\")\n",
    "create_table_df = spark.sql(f\"CREATE TABLE IF NOT EXISTS {FRAUD_DETECTION_SRC_TBL} ({final_output_table_columns_str}) STORED AS PARQUET LOCATION '{FRAUD_DETECTION_OUTPUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63bc1569-8ab8-4c02-a951-c512cf10305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_response_df = spark.sql(f\"INSERT OVERWRITE {FRAUD_DETECTION_SRC_TBL} select * from imputed_final_tmp_tbl distribute by 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62906936-b247-4fc8-aa46-a8507de42450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  590540|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select count(*) from {FRAUD_DETECTION_SRC_TBL}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ee384-23fd-49f9-be95-8688a7a921af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
